# -*- coding: utf-8 -*-
"""Layer 2. Causal Encoder Module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDl6NG4-xOqVFs8nDTmPT8KwbehFkoPx
"""

import torch
import torch.nn as nn

class CrossAttentionModule(nn.Module):
    def __init__(self, hidden_dim):
        super(CrossAttentionModule, self).__init__()
        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=0.1)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.residual_connection = nn.Identity()

    def forward(self, key, query, context):
        # Cross-attention between key, query, and context
        attn_output, _ = self.cross_attention(query, key, context)

        # Adding residual connection
        attn_output = self.residual_connection(attn_output + query)

        # Layer normalization
        attn_output = self.layer_norm(attn_output)

        return attn_output

class ComprehensiveEffectRepresentation(nn.Module):
    def __init__(self, hidden_dim):
        super(ComprehensiveEffectRepresentation, self).__init__()
        self.cross_att_kec = CrossAttentionModule(hidden_dim)
        self.cross_att_kes = CrossAttentionModule(hidden_dim)

    def forward(self, KEC_intra, KEC_inter, Hc, Hq):
        # Concatenate intra and inter emotion effects to form KEC
        KEC = torch.cat((KEC_intra, KEC_inter), dim=-1)

        # Apply cross-attention with cause-aware context
        KEC_hat = self.cross_att_kec(KEC, Hc, Hc)  # Cross-attention for KEC
        KES_hat = self.cross_att_kes(KEC, Hq, Hq)  # Cross-attention for KES

        return KEC_hat, KES_hat

# Example usage
hidden_dim = 512  # Adjust based on your model's requirements
cross_attention_module = ComprehensiveEffectRepresentation(hidden_dim)

# Dummy tensors for demonstration
K_EC_intra = torch.randn(10, 32, hidden_dim)  # [sequence length, batch size, hidden dim]
K_EC_inter = torch.randn(10, 32, hidden_dim)
H_c = torch.randn(10, 32, hidden_dim)
H_q = torch.randn(10, 32, hidden_dim)

# Forward pass
K_EC_hat, K_ES_hat = cross_attention_module(K_EC_intra, K_EC_inter, H_c, H_q)
print(KEC_hat.shape, KES_hat.shape)

import torch

def compute_query_vector(Hs, Hc):
    """
    Compute the query vector h by mean-pooling the strategy history and context representation,
    then concatenating the results.

    Args:
        Hs (torch.Tensor): Strategy history tensor of shape [sequence_length, batch_size, hidden_dim].
        Hc (torch.Tensor): Context representation tensor of shape [sequence_length, batch_size, hidden_dim].

    Returns:
        h (torch.Tensor): Query vector of shape [batch_size, 2 * hidden_dim].
    """
    # Mean-pooling over the sequence length dimension
    s = torch.mean(Hs, dim=0)  # Shape: [batch_size, hidden_dim]
    c = torch.mean(Hc, dim=0)  # Shape: [batch_size, hidden_dim]

    # Concatenate the mean-pooled representations
    h = torch.cat((s, c), dim=-1)  # Shape: [batch_size, 2 * hidden_dim]

    return h

# Example usage
Hs = torch.randn(10, 32, 512)  # [sequence_length, batch_size, hidden_dim]
Hc = torch.randn(10, 32, 512)  # [sequence_length, batch_size, hidden_dim]

# Compute the query vector
h = compute_query_vector(Hs, Hc)
print(h.shape)  # Should output: [32, 1024]

