# -*- coding: utf-8 -*-
"""Layer 3 Independent Integrated Strategy Executors Module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDl6NG4-xOqVFs8nDTmPT8KwbehFkoPx
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Load the model and tokenizer
model_name = "facebook/blenderbot-400M-distill"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Strategy descriptions
strategy_descriptions = {
    "Question": "Asking for information related to the problem to help the help-seeker articulate the issues that they face. Open-ended questions are best, and closed questions can be used to get specific information.",
    "Restatement or Paraphrasing": "A simple, more concise rephrasing of the help-seeker’s statements that could help them see their situation more clearly.",
    "Reflection of Feelings": "Articulate and describe the help-seeker’s feelings.",
    "Self-disclosure": "Divulge similar experiences that you have had or emotions that you share with the help-seeker to express your empathy.",
    "Affirmation and Reassurance": "Affirm the help-seeker’s strengths, motivation, and capabilities and provide reassurance and encouragement.",
    "Providing Suggestions": "Provide suggestions about how to change the situation, but be careful to not overstep and tell them what to do.",
    "Information": "Provide useful information to the help-seeker, for example with data, facts, opinions, resources, or by answering questions.",
    "Others": "Exchange pleasantries and use other support strategies that do not fall into the above categories.",
}

# Encode descriptions into embeddings
def encode_descriptions(descriptions, tokenizer, model):
    embeddings = {}
    model.eval()
    with torch.no_grad():
        for strategy, text in descriptions.items():
            # Tokenize and encode the text
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

            # Get encoder outputs using model.model.encoder instead of model.encoder
            outputs = model.model.encoder(**inputs)

            embeddings[strategy] = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings
    return embeddings

# Get embeddings for all strategies
strategy_embeddings = encode_descriptions(strategy_descriptions, tokenizer, model)

# Print embeddings
for strategy, embedding in strategy_embeddings.items():
    print(f"Strategy: {strategy}, Embedding Shape: {embedding.shape}")

import torch
import torch.nn as nn
import torch.nn.functional as F

class IndependentIntegratedExecutors(nn.Module):
    def __init__(self, hidden_dim, num_strategies):
        super(IndependentIntegratedExecutors, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_strategies = num_strategies

        # Independent executors: one cross-attention module for each strategy
        self.strategy_executors = nn.ModuleList([
            nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=0.1)
            for _ in range(num_strategies)
        ])

        # Fusion layer for integrating strategies
        self.strategy_fusion = nn.Linear(hidden_dim * num_strategies, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)

        # Decoder for final output
        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8)

    def forward(self, O, X, H_D, strategy_distribution):
        """
        Args:
            O: Decoder hidden state (T_dec, B, H)
            X: Comprehensive causal information (T_enc, B, H)
            H_D: Strategy descriptions (T_desc, B, H)
            strategy_distribution: Strategy weights (B, num_strategies)
        Returns:
            O_prime: Updated decoder hidden state (T_dec, B, H)
        """
        executor_outputs = []
        # Permute the dimensions of H_D to (num_strategies, batch_size, hidden_dim)
        # This aligns the batch size dimension for concatenation with X
        H_D = H_D.permute(1, 0, 2)

        # Independent strategy execution
        for i, executor in enumerate(self.strategy_executors):
            # Now H_D[i] will have the shape (batch_size, hidden_dim)
            strategy_specific_info = torch.cat((X, H_D[i, :].unsqueeze(0)), dim=0)  # Concatenate X and H_D_i
            O_E_i, _ = executor(O, strategy_specific_info, strategy_specific_info)
            executor_outputs.append(O_E_i)

            # Combine outputs from all executors
            executor_outputs = torch.stack(executor_outputs, dim=1)  # Shape: (T_dec, num_strategies, B, H)
            weighted_executor_output = torch.einsum(
            'bs,tsbh->tbh', strategy_distribution, executor_outputs
            )  # Weighted sum of executors based on strategy distribution

            # Layer normalization and residual connection
            O_E = self.layer_norm(O + weighted_executor_output)

            # Final decoding with integrated strategies
            O_prime = self.decoder(O_E, X)

            return O_prime

# Example Usage
if __name__ == "__main__":
    hidden_dim = 256
    num_strategies = 8
    batch_size = 16
    seq_len_dec = 20
    seq_len_enc = 50

    model = IndependentIntegratedExecutors(hidden_dim, num_strategies)

    # Dummy Inputs
    O = torch.randn(seq_len_dec, batch_size, hidden_dim)
    X = torch.randn(seq_len_enc, batch_size, hidden_dim)
    H_D = torch.randn(seq_len_dec, num_strategies, hidden_dim)
    strategy_distribution = torch.softmax(torch.randn(batch_size, num_strategies), dim=-1)

    # Forward Pass
    output = model(O, X, H_D, strategy_distribution)
    print(output.shape)  # Expected: (seq_len_dec, batch_size, hidden_dim)

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BlenderbotSmallTokenizer, BlenderbotSmallForConditionalGeneration

# Define the model components
class EmotionalSupportModel(nn.Module):
    def __init__(self, input_dim, blenderbot_model_name="facebook/blenderbot_small-90M"):
        super(EmotionalSupportModel, self).__init__()

        # Hidden dimension
        self.hidden_dim = input_dim

        # BlenderBot decoder as DEC
        self.decoder = BlenderbotSmallForConditionalGeneration.from_pretrained(blenderbot_model_name)
        self.tokenizer = BlenderbotSmallTokenizer.from_pretrained(blenderbot_model_name)

        # LayerNorm
        self.layer_norm = nn.LayerNorm(input_dim)

    def forward(self, O, O_E_list, p_i, context_input):
        """
        Parameters:
        O: torch.Tensor - Original input embeddings of shape (batch_size, hidden_dim)
        O_E_list: list of torch.Tensor - List of outputs O_E_i (n tensors of shape (batch_size, hidden_dim))
        p_i: torch.Tensor - Weights for summation, shape (n,)
        context_input: str - Text input for the decoder

        Returns:
        O_E: torch.Tensor - Layer normalized combined embedding
        O_prime: torch.Tensor - Decoder output
        """
        # Calculate Z_E = Σ (p_i * O_E_i)
        Z_E = torch.zeros_like(O)
        for i, O_E_i in enumerate(O_E_list):
            Z_E += p_i[i] * O_E_i  # Weighted summation

        # Apply LayerNorm: O_E = LayerNorm(O + Z_E)
        O_E = self.layer_norm(O + Z_E)

        # Prepare the context input for the BlenderBot decoder
        context_tokens = self.tokenizer(context_input, return_tensors="pt", padding=True, truncation=True)
        decoder_input_ids = context_tokens.input_ids

        # Pass O_E and the context to the decoder
        O_prime = self.decoder(
            input_ids=decoder_input_ids,
            encoder_hidden_states=O_E.unsqueeze(0)  # Add batch dimension
        ).logits

        return O_E, O_prime

# Dummy Example Usage
if __name__ == "__main__":
    # Hyperparameters
    input_dim = 128
    batch_size = 1
    n = 3  # Number of O_E_i

    # Initialize the model
    model = EmotionalSupportModel(input_dim=input_dim)

    # Example tensors
    O = torch.randn(batch_size, input_dim)  # Original embeddings
    O_E_list = [torch.randn(batch_size, input_dim) for _ in range(n)]  # List of O_E_i
    p_i = torch.tensor([0.3, 0.5, 0.2])  # Weights (must sum to 1)

    # Context input for the decoder
    context_input = "This is an example context input for the decoder."

    # Forward pass
    O_E, O_prime = model(O, O_E_list, p_i, context_input)

    # Output shapes
    print("O_E shape:", O_E.shape)  # Should be (batch_size, hidden_dim)
    print("O_prime shape:", O_prime.shape)  # Should be (batch_size, sequence_length, vocab_size)